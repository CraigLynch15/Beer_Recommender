{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deploy_Beer_Recommendations_Streamlet_Full_V02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4GhsvnkonKh",
        "outputId": "cda38e8a-210b-4f9d-c362-68d7d1253158"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install surprise\n",
        "from surprise import Reader, Dataset\n",
        "from surprise import SVD, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, SVDpp\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from surprise import accuracy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1619392 sha256=3dbca3654cf20327572f2632fe9d8a0b1ea8be239bbe5a651ac4e1fdf650f3dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdaiJL7moqe5",
        "outputId": "23e014ac-afe6-44f2-99ec-1215e01b3ea7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_read = pd.read_csv('/content/drive/My Drive/LHL_Final_Project/Written_Reviews/Beer_Recommendations_EDA_V01.csv')\n",
        "data_read.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(853486, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDNKkFPFotjH"
      },
      "source": [
        "df = data_read.sample(frac=0.10, random_state=42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohhGfgw5o2jN"
      },
      "source": [
        "df = df.rename(columns={'username': 'userID','Name':'itemID','user_overall_score':'rating'})\n",
        "df_copy = df.copy()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QqCBYE8tcCb",
        "outputId": "3073a070-83b2-4458-9060-199bb80e34d8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import re\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "data_read = pd.read_csv('/content/drive/My Drive/LHL_Final_Project/Written_Reviews/Beer_Recommendations_EDA_V01.csv')\n",
        "df = data_read.copy()\n",
        "df = df.sample(frac=0.10, random_state=1)\n",
        "\n",
        "# Remove Punctuation\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "df['text'] = df['text'].apply(remove_punctuations)\n",
        "\n",
        "# Make Strings Lowercase\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Remove Digits\n",
        "df['text'] = df['text'].str.replace('\\d+', '')\n",
        "\n",
        "#define lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['without_stopwords']  = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "#lemmatize/tokenize/make everything lowercase/remove punctuation\n",
        "df['lemmatized_text'] = df['without_stopwords'].apply(\\\n",
        "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
        "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x)))]))\n",
        "\n",
        "#remove digitsvb\n",
        "df['lemmatized_text'] = df['lemmatized_text'].str.replace('\\d+', '')\n",
        "\n",
        "#add some more stopwords\n",
        "newStopWords = ['beer', 'one', 'would', 'get', 'come', 'also', 'oz', 'could']\n",
        "stop.extend(newStopWords)\n",
        "df['lemmatized_text'] = df['lemmatized_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "df['lemmatized_text'] = df['lemmatized_text'].astype('str')\n",
        "\n",
        "\n",
        "# add some more stopwords\n",
        "newStopWords = ['lager', 'porter', 'stout', 'brewery', 'ale', 'apa', 'ipa', 'ipas', 'belgian', 'red', 'brown', 'cream', 'black', 'amber',\n",
        "               'golden', 'much', 'year', 'worth', 'english', 'german', 'american', 'stuff', 'pilsner', 'old', 'barleywine',\n",
        "               'ri', 'imperial', 'non', 'dipa', 'iipa', 'dipas', 'pils', 'pilsener', 'sam', 'irish', 'brewpub', 'st',\n",
        "               'adam', 'ml', 'tripel', 'quad', 'pa', 'ibu', 'ibus', 'series', 'bell', 'belgium', 'boston', 'city', 'coors',\n",
        "               'dead', 'dfh', 'dog', 'dogfish', 'founder', 'im', 'imo', 'lagunitas', 'left', 'nevada', 'rogue', 'samuel', 'sierra',\n",
        "               'southern', 'pint']\n",
        "stop.extend(newStopWords)\n",
        "df['lemmatized_text'] = df['lemmatized_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "df['lemmatized_text'] = df['lemmatized_text'].astype('str')\n",
        "\n",
        "def nouns_adj(text):\n",
        "    #Given a string of text, tokenize the text and pull out only the nouns and adjectives.\n",
        "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "    tokenized = word_tokenize(text)\n",
        "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
        "    return ' '.join(nouns_adj)\n",
        "\n",
        "df['adjectives/nouns'] = df['lemmatized_text'].apply(nouns_adj)\n",
        "df = df.dropna()\n",
        "\n",
        "#create new dataframe\n",
        "beer_text = df[['beer_id', 'beer_name', 'style', 'brewery_name', 'Name', 'lemmatized_text', 'adjectives/nouns']]\n",
        "\n",
        "#drop null values\n",
        "beer_text = df.dropna(axis=0, subset=['adjectives/nouns'])\n",
        "\n",
        "#group by beer, think of each beer as a \"document\"\n",
        "grouped_beer_text = beer_text.groupby(['beer_id', 'brewery_name', 'beer_name', 'Name', 'style'])['adjectives/nouns'].agg(lambda col: ''.join(col))\n",
        "\n",
        "#df\n",
        "beer_text = pd.DataFrame(grouped_beer_text)\n",
        "beer_text = beer_text.reset_index()\n",
        "\n",
        "# using lemmatized_text, create the corpus\n",
        "corpus = beer_text['adjectives/nouns']\n",
        "\n",
        "# FEATURE EXTRACTION via tfidf vectorizer\n",
        "tfidf_model = TfidfVectorizer(max_features=500,\n",
        "                             max_df=0.25,\n",
        "                             min_df=0.01)\n",
        "\n",
        "tfidf_matrix = tfidf_model.fit_transform(corpus).todense()\n",
        "print('The shape of the matrix is:', tfidf_matrix.shape)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix)\n",
        "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
        "\n",
        "# Calculate the cosine similarity of the matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Construct a reverse mapping of indices and beer names, and drop duplicate names, if any\n",
        "indices = pd.Series(beer_text.index, index=beer_text['Name']).drop_duplicates()\n",
        "\n",
        "print(type(indices))\n",
        "print(type(cosine_sim))\n",
        "\n",
        "indices.to_pickle('indices')\n",
        "np.save(\"cosine_sim.npy\", cosine_sim)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "The shape of the matrix is: (1709, 500)\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TjhGy_kxsdKn",
        "outputId": "dec2b9c9-ed2c-4aa6-8db5-e8db59daf990"
      },
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-0.88.0-py2.py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.0-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.5-py3-none-manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.3.1-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.6.4)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.12.3)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
            "Collecting ipython<8.0,>=7.23.1\n",
            "  Downloading ipython-7.27.0-py3-none-any.whl (787 kB)\n",
            "\u001b[K     |████████████████████████████████| 787 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.5.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.11.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=9d2b290470be759bceed5a1a5cb2bad15d674a08a6379c62133c085bc819f426\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, smmap, gitdb, watchdog, validators, pydeck, gitpython, blinker, base58, streamlit\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.3.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.27.0 which is incompatible.\u001b[0m\n",
            "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.18 ipykernel-6.3.1 ipython-7.27.0 prompt-toolkit-3.0.20 pydeck-0.7.0 smmap-4.0.0 streamlit-0.88.0 validators-0.18.2 watchdog-2.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19006 sha256=b6e22949e5d9a5504066870a9322cb794a3fa1c14af1676b4e729d21717decb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbLte8-2o5e3",
        "outputId": "645bec3a-8d94-4300-b869-19209709ec2a"
      },
      "source": [
        "%%writefile full.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from surprise import Reader, Dataset\n",
        "from surprise import SVD, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, SVDpp\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from collections import defaultdict\n",
        "from surprise import accuracy\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import random\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "\n",
        "\n",
        "st.set_page_config(layout=\"wide\") \n",
        "st.title('Beer Recommendations')\n",
        "\n",
        "page_names = ['Existing User', 'New User', 'Help Me Choose']\n",
        "\n",
        "page = st.radio('Navigation', page_names)\n",
        "\n",
        "st.write(page)\n",
        "\n",
        "if page == 'New User':\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  data_read = pd.read_csv('/content/drive/My Drive/LHL_Final_Project/Written_Reviews/Beer_Recommendations_EDA_V01.csv')\n",
        "  # User 10% of the dataset for testing\n",
        "  df_item_based = data_read.sample(frac=0.10, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "  st.header('Beer Recommendation App - Collaboration Filter')\n",
        "  st.write('This app was built to help you expand your beer horizons')\n",
        "  st.write('Data obtained from https://www.kaggle.com/ehallmar/beers-breweries-and-beer-reviews')\n",
        "\n",
        "  beer1 = 'Molson Canadian Lager'\n",
        "  beer2 = 'Guinness Extra Stout Original'\n",
        "  beer3 = 'Blue Moon Belgian White'\n",
        "  beer4 = '90 Minute IPA'\n",
        "  beer5 = 'Nut Brown Ale'\n",
        "  beer6 = 'Smoked Porter'\n",
        "  beer7 = 'Gose'\n",
        "\n",
        "\n",
        "\n",
        "  # User input for new beer\n",
        "  st.title('Rate these beer on a scale of 0-5. If you have not tried one, rate how interested you are in trying it:')\n",
        "  with st.form(key='form1'):\n",
        "    st.write('Molson Canadian Lager')\n",
        "    rating1 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='1')\n",
        "    st.write('Guinness Extra Stout Original')\n",
        "    rating2 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='2')\n",
        "    st.write('Blue Moon Belgian White')\n",
        "    rating3 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='3')\n",
        "    st.write('90 Minute IPA')\n",
        "    rating4 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='4')\n",
        "    st.write('Nut Brown Ale')\n",
        "    rating5 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='5')\n",
        "    st.write('Smoked Porter')\n",
        "    rating6 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='6')\n",
        "    st.write('Gose')\n",
        "    rating7 = st.slider(\"Rating\", min_value=0.0, max_value=5.0, step=0.25,key='7')\n",
        "    \n",
        "    submit = st.form_submit_button(label = 'Confirm Ratings')\n",
        "\n",
        "  if st.button('Press Here For New Beer!'):\n",
        "\n",
        "    text = ['the perfect beer for anyone who loves the summer months hanging with friends on patios and pairing anything barbecue with refreshing brews',\n",
        "            'the perfect beer for anyone who loves the bitter flavour of black coffee or a strong grapefruit',\n",
        "            'the perfect beer for anyone who loves coffee-crisp purposely burnt marshmallows and other bold rich flavours',\n",
        "            'the perfect beer for anyone who loves dried fruit Werther’s Original candy bold flavours and just a pinch of sweet flavours',\n",
        "            'the perfect beer for anyone who loves scotch campfires and smoked meats smoky brews can be enjoyed all year round',\n",
        "            'the perfect beer for anyone whos morning routine consists of chugging a glass of orange juice or anyone who never says when when the waiter is adding pepper to your meal',\n",
        "            'the perfect beers for anyone who drinks green tea in the morning instead of coffee puts cucumber in their water to give it flavour or is semi-addicted to sour penny candies']\n",
        "\n",
        "    user = [(beer1,rating1),(beer2,rating2),(beer3,rating3),(beer4,rating4),(beer5,rating5),(beer6,rating6),(beer7,rating7)]\n",
        "    user = pd.DataFrame(user, columns = ['beer_name', 'user_overall_score'])\n",
        "\n",
        "    user['brewery_name'] =  pd.np.where(user.beer_name.str.contains(\"Canadian\"), 'Molson Coors Canada',\n",
        "                            pd.np.where(user.beer_name.str.contains(\"Guinness\"), 'Guinness Ltd',\n",
        "                            pd.np.where(user.beer_name.str.contains(\"Blue Moon\"), 'Coors Brewing Company MolsonCoors',\n",
        "                            pd.np.where(user.beer_name.str.contains(\"90 Minute\"), 'Dogfish Head Craft Brewery',\n",
        "                            pd.np.where(user.beer_name.str.contains(\"Nut Brown Ale\"), 'Samuel Smith Old Brewery Tadcaster',\n",
        "                            pd.np.where(user.beer_name.str.contains(\"Smoked Porter\"), 'Alaskan Brewing Co Alaskan',\n",
        "                            pd.np.where(user.beer_name.str.contains(\"Flemish\"), 'Brouwerij Van Steenberge NV', 'Westbrook Brewing Co')))))))\n",
        "\n",
        "    user['beer_id'] = pd.np.where(user.beer_name.str.contains(\"Canadian\"), '1312.0',\n",
        "                      pd.np.where(user.beer_name.str.contains(\"Guinness\"), '650.0',\n",
        "                      pd.np.where(user.beer_name.str.contains(\"Blue Moon\"), '1212.0',\n",
        "                      pd.np.where(user.beer_name.str.contains(\"90 Minute\"), '2093.0',\n",
        "                      pd.np.where(user.beer_name.str.contains(\"Nut Brown Ale\"), '576.0',\n",
        "                      pd.np.where(user.beer_name.str.contains(\"Smoked Porter\"), '90.0',\n",
        "                      pd.np.where(user.beer_name.str.contains(\"Flemish\"), '10482.0', '133043.0')))))))\n",
        "\n",
        "    user['text'] = pd.np.where(user.beer_name.str.contains(\"Lager\"), text[0],\n",
        "                  pd.np.where(user.beer_name.str.contains(\"IPA\"), text[1],\n",
        "                  pd.np.where(user.beer_name.str.contains(\"Stout\"), text[2],\n",
        "                  pd.np.where(user.beer_name.str.contains(\"Brown Ale\"), text[3],\n",
        "                  pd.np.where(user.beer_name.str.contains(\"Smoke\"), text[4],\n",
        "                  pd.np.where(user.beer_name.str.contains(\"White\"), text[5], text[6]))))))\n",
        "\n",
        "    new_ratings_df = df_item_based.append(user,ignore_index=True)\n",
        "    new_ratings_df = new_ratings_df.fillna(0)\n",
        "    new_ratings_df = new_ratings_df[['username','Name','user_overall_score']]\n",
        "    new_ratings_df['user_overall_score'] = new_ratings_df['user_overall_score'].astype(float, errors = 'raise')\n",
        "    ratings = new_ratings_df.pivot_table(index='username', columns='Name', values='user_overall_score')\n",
        "    ratings.fillna(0,inplace=True)\n",
        "\n",
        "    # Removing sparsity \n",
        "    csr_ratings = csr_matrix(ratings.values)\n",
        "    # Standardize the mean of each user to 0, and correct for harsh/lenient users\n",
        "    def standardize(row):\n",
        "      new_row = (row - row.mean()) / (row.max() - row.min())\n",
        "      return new_row\n",
        "\n",
        "    ratings_std = ratings.apply(standardize)\n",
        "\n",
        "    # For item-to-item Similarity\n",
        "    item_similarity = cosine_similarity(ratings_std.T)\n",
        "    item_similarity_df = pd.DataFrame(item_similarity,index=ratings_std.columns, columns=ratings_std.columns)\n",
        "    \n",
        "\n",
        "\n",
        "    def get_similar_beer(beer_name,user_rating):\n",
        "      similar_score = item_similarity_df[beer_name]*(user_rating - 2.5)\n",
        "      similar_score = similar_score.sort_values(ascending = False)\n",
        "      return similar_score\n",
        "\n",
        "    # Using user input beer\n",
        "    user['Name'] = user['brewery_name']+' '+user['beer_name']\n",
        "    new_user = user[['Name', 'user_overall_score']]\n",
        "    new_user_list = new_user.to_records(index=False).tolist()\n",
        "\n",
        "    similar_beer = pd.DataFrame()\n",
        "\n",
        "    for beer, rating in new_user_list:\n",
        "      similar_beer = similar_beer.append(get_similar_beer(beer,rating))\n",
        "      x = similar_beer.sum().sort_values(ascending= False)\n",
        "      x = pd.DataFrame(x).reset_index()\n",
        "      x = x.rename(columns={'index': 'Name',0:'Score'})\n",
        "      y = pd.DataFrame(new_user_list)\n",
        "      y = y.rename(columns={0: 'Name',1:'Score'})\n",
        "      x = x[~x['Name'].isin(y['Name'])]\n",
        "      \n",
        "    # top_choices = x.head(10)\n",
        "    # bottom_choices = x.tail(1)\n",
        "    st.write('Top Beer Choices')\n",
        "    st.write(x['Name'].head(10))\n",
        "    st.write('Or Maybe Something Completely Different')\n",
        "    st.write(x['Name'].tail(1))\n",
        "\n",
        "elif page == 'Existing User':\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  data_read = pd.read_csv('/content/drive/My Drive/LHL_Final_Project/Written_Reviews/Beer_Recommendations_EDA_V01.csv')\n",
        "  df = data_read.sample(frac=0.10, random_state=42)\n",
        "  df = df.rename(columns={'username': 'userID','Name':'itemID','user_overall_score':'rating'})\n",
        "  df_copy = df.copy()\n",
        "  df_copy['Name'] = df_copy['brewery_name']+' '+df_copy['beer_name']\n",
        "  df_copy = df_copy[['userID', 'Name', 'itemID', 'style', 'rating']]\n",
        "\n",
        "  reader = Reader(rating_scale=(0, 5))\n",
        "  df = Dataset.load_from_df(df[['userID', \t'itemID', \t'rating' \t]], reader=reader)\n",
        "  # Creating a train/test set within Surprise\n",
        "  raw_ratings = df.raw_ratings\n",
        "  threshold = int(.7 * len(raw_ratings))                                     \n",
        "  trainset_raw_ratings = raw_ratings[:threshold]                             \n",
        "  test_raw_ratings = raw_ratings[threshold:]  \n",
        "  df.raw_ratings = trainset_raw_ratings\n",
        "\n",
        "  algo_SVD = SVD(n_factors = 4)\n",
        "  trainset = df.build_full_trainset()\n",
        "  algo_SVD.fit(trainset)\n",
        "  print('algorithm: SVD with GridSearchCV')\n",
        "  # Trainset\n",
        "  predictions = algo_SVD.test(trainset.build_testset())\n",
        "  print('Biased accuracy on trainset,', end='   ')\n",
        "  accuracy.rmse(predictions)\n",
        "  # Testset\n",
        "  testset = df.construct_testset(test_raw_ratings)\n",
        "\n",
        "  predictions = algo_SVD.test(testset)\n",
        "  print('Unbiased accuracy on testset,', end=' ')\n",
        "  accuracy.rmse(predictions)\n",
        "\n",
        "\n",
        "  def get_top_n(predictions, userID, df_copy, n = 10):\n",
        "      '''Return the top N (default) itemID for a user,.i.e. userID and history for comparison\n",
        "      Args:\n",
        "      Returns: \n",
        "    \n",
        "      '''\n",
        "      #Part I.: Surprise docomuntation\n",
        "      \n",
        "      #1. First map the predictions to each user.\n",
        "      top_n = defaultdict(list)\n",
        "      for uid, iid, true_r, est, _ in predictions:\n",
        "          top_n[uid].append((iid, est))\n",
        "\n",
        "      #2. Then sort the predictions for each user and retrieve the k highest ones.\n",
        "      for uid, user_ratings in top_n.items():\n",
        "          user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
        "          top_n[uid] = user_ratings[: n ]\n",
        "      \n",
        "      #Part II.: inspired by: https://beckernick.github.io/matrix-factorization-recommender/\n",
        "      \n",
        "      #3. Tells how many beer the user has already rated\n",
        "      user_data = df_copy[df_copy.userID == (userID)]\n",
        "      print('User {0} has already rated {1} beer.'.format(userID, user_data.shape[0]))\n",
        "\n",
        "      \n",
        "      #4. Data Frame with predictions. \n",
        "      preds_df = pd.DataFrame([(id, pair[0],pair[1]) for id, row in top_n.items() for pair in row],\n",
        "                          columns=[\"userID\" ,\"itemID\",\"rat_pred\"])\n",
        "      \n",
        "      \n",
        "      #5. Return pred_usr, i.e. top N recommended beer. \n",
        "      pred_usr = pd.merge(preds_df[preds_df[\"userID\"] == (userID)],df_copy, on = 'itemID')\n",
        "      pred_usr = pred_usr.rename(columns={'userID_x': 'userID'})\n",
        "      pred_usr = pred_usr[['userID', 'itemID', 'style', 'rat_pred']]\n",
        "              \n",
        "      #6. Return hist_usr, i.e. top N historically rated beer.\n",
        "      hist_usr = df_copy[['userID','itemID','style','rating']]\n",
        "      hist_usr = hist_usr[hist_usr['userID'] == userID]\n",
        "      \n",
        "      \n",
        "      return hist_usr, pred_usr\n",
        "\n",
        "\n",
        "  st.header('Recommendations for Existing Users')\n",
        "\n",
        "  # Sample of usernames for testing\n",
        "  st.write('Sample Usernames')\n",
        "  st.dataframe(df_copy['userID'].sample(20))\n",
        "\n",
        "  with st.form(key='form1'):  \n",
        "    username = st.text_input('Select Username')\n",
        "    st.form_submit_button(label = 'Press Here for New Beer!')\n",
        "    hist_SVD, pred_SVD = get_top_n(predictions, df_copy = df_copy, userID = username)\n",
        "\n",
        "\n",
        "  st.write('Displaying Recommendations for User: ', username)\n",
        "\n",
        "  pred_SVD = pred_SVD.drop_duplicates('itemID')\n",
        "  hist_SVD = hist_SVD.drop_duplicates('itemID')\n",
        "  st.write('Recommendations')\n",
        "  st.dataframe(pred_SVD)\n",
        "  st.write('Previous Ratings')\n",
        "  st.dataframe(hist_SVD)\n",
        "\n",
        "else:\n",
        "\n",
        "  import pickle\n",
        "  indices = pd.read_pickle('indices')\n",
        "  cosine_sim = np.load(\"cosine_sim.npy\")\n",
        "  beer_text = pd.read_csv('/content/drive/My Drive/LHL_Final_Project/Written_Reviews/beer_text.csv')\n",
        "\n",
        "\n",
        "  st.title('Beer Recommendation App - Content Recommendation')\n",
        "  st.write('This app was built to help you expand your beer horizons')\n",
        "  st.write('Data obtained from https://www.kaggle.com/ehallmar/beers-breweries-and-beer-reviews')\n",
        "\n",
        "\n",
        "  def content_recommender(title, cosine_sim=cosine_sim, df=beer_text, indices=indices):\n",
        "      # Obtain the index of the beer that matches the name\n",
        "      idx = indices[title]\n",
        "      sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "      sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "      sim_scores = sim_scores[1:11]\n",
        "      beer_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "      \n",
        "      return beer_text[['style', 'brewery_name', 'beer_name']].iloc[beer_indices]\n",
        "\n",
        "  st.write('What are you in the mood for?')\n",
        "\n",
        "  want = st.selectbox('Select', ['I want a beer for summer, hanging with friends on patios, and barbecueing.',\n",
        "          'I love the bitter flavour of black coffee or a strong grapefruit.',\n",
        "          'I love coffee-crisp, purposely burnt marshmallows, and other bold rich flavours.',\n",
        "          'I love dried fruit, Werther’s Originals, bold flavours, and just a pinch of sweet.',\n",
        "          'I love scotch, campfires, and smoked meats.',\n",
        "          \"I love a glass of orange juice and I never says 'when' when the waiter is adding pepper to my meal.\",\n",
        "          'I drink green tea instead of coffee, love cucumber in my water, and am semi-addicted to sour candies'])\n",
        "  if 'barbecue' in want:\n",
        "    test_beer = 'Molson Coors Canada Molson Canadian Lager'\n",
        "  elif 'marshmallows' in want:\n",
        "    test_beer = 'Guinness Ltd Guinness Extra Stout Original'\n",
        "  elif 'chugging' in want:\n",
        "    test_beer = 'Coors Brewing Company MolsonCoors Blue Moon Belgian White'\n",
        "  elif 'grapefruit' in want:\n",
        "    test_beer = 'Dogfish Head Craft Brewery 90 Minute IPA'\n",
        "  elif 'Original' in want:\n",
        "    test_beer = 'Samuel Smith Old Brewery Tadcaster Nut Brown Ale'\n",
        "  elif 'smoked' in want:\n",
        "    test_beer = 'Alaskan Brewing Co Alaskan Smoked Porter'\n",
        "  elif 'cucumber' in want:\n",
        "    test_beer = 'Westbrook Brewing Co Gose'\n",
        "  else:\n",
        "    test_beer = \"I'm not a beer person\"\n",
        "\n",
        "\n",
        "  st.write(content_recommender(test_beer))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting full.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziEkD0tXpmim",
        "outputId": "e0366033-d6f7-4099-cebc-0091cc9ec0cf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine_sim.npy\tdrive  full.py\tindices  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMgzLsgapmtO",
        "outputId": "528165bf-729c-4330-f6b5-2b314e439d4d"
      },
      "source": [
        "import os\n",
        "!ngrok authtoken 1xYDWGv7QBn67ewsPb48Fn5GMvC_7XmCARHHAu6VwMpSoHrXb\n",
        "from pyngrok import ngrok\n",
        "!streamlit run --server.port 80 full.py&>/dev/null&\n",
        "publ_url = ngrok.connect(port='80')\n",
        "publ_url "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://8463-34-125-6-203.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT5WoN60-TFu"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb-yD5S4-TII"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBL6L2zn-TKx"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoX3pjCt-TNP"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SecESgCP-TQV"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E2Hl8Kwpmvu"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyimU4dts1Th"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}